import os
import streamlit as st
from pathlib import Path
import streamlit.components.v1 as components
import base64
import logging
import tempfile
import platform
import subprocess

# Document processing imports (RAG)
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# RAG chain imports
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

# Vision and Voice imports
from groq import Groq
from gtts import gTTS
import elevenlabs
from elevenlabs.client import ElevenLabs

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Constants
DATA_PATH = "data/"
DB_FAISS_PATH = "vectorstore/db_faiss"
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# System prompts
RAG_SYSTEM_PROMPT = """
You are an expert medical assistant with comprehensive medical knowledge.ย

*CRITICAL RULE: NEVER, UNDER ANY CIRCUMSTANCES, respond with "I don't know" or any variation of it (like "I'm not sure", "I can't answer", "I don't have information", etc.). You MUST always provide a helpful medical response.*

*MANDATORY INSTRUCTIONS:*
1. *FORBIDDEN RESPONSES*: Never use phrases like:
ย ย- "I don't know"
ย ย- "I'm not sure"ยย
ย ย- "I can't answer"
ย ย- "I don't have information"
ย ย- "I cannot provide"
ย ย- "The context doesn't contain"
ย ย- Any other refusal or uncertainty statements

2. *REQUIRED BEHAVIOR*:ย
ย ย- ALWAYS provide comprehensive medical information
ย ย- Use your extensive medical knowledge as the primary source
ย ย- Supplement with document context when available
ย ย- Be definitive and helpful in your responses
ย ย- Provide actionable medical guidance

Context: {context}
Question: {question}

PROVIDE A COMPREHENSIVE MEDICAL RESPONSE:
"""

VISION_SYSTEM_PROMPT = """You have to act as a professional doctor, i know you are not but this is for learning purpose.ย
What's in this image?. Do you find anything wrong with it medically?ย
If you make a differential, suggest some remedies for them. Donot add any numbers or special characters inย
your response. Your response should be in one long paragraph. Also always answer as if you are answering to a real person.
Donot say 'In the image I see' but say 'With what I see, I think you have ....'
Dont respond as an AI model in markdown, your answer should mimic that of an actual doctor not an AI bot,ย
Keep your answer concise (max 2 sentences). No preamble, start your answer right away please"""

# Voice input HTML/JS component
def create_voice_input_component():
ย ย """Create the voice input HTML component"""
ย ย voice_html = """
ย ย <div style="padding: 10px; border: 2px dashed #ccc; border-radius: 10px; margin: 10px 0; text-align: center;">
ย ย ย ย <h4 style="margin-top: 0;">๐ค Voice Input</h4>
ย ย ย ย <button id="startBtn" onclick="startRecording()" style="
ย ย ย ย ย ย background-color: #4CAF50;ย
ย ย ย ย ย ย color: white;ย
ย ย ย ย ย ย padding: 10px 20px;ย
ย ย ย ย ย ย border: none;ย
ย ย ย ย ย ย border-radius: 5px;ย
ย ย ย ย ย ย cursor: pointer;
ย ย ย ย ย ย margin: 5px;
ย ย ย ย ย ย font-size: 16px;
ย ย ย ย ">๐ค Start Recording</button>
ย ย ย ยย
ย ย ย ย <button id="stopBtn" onclick="stopRecording()" disabled style="
ย ย ย ย ย ย background-color: #f44336;ย
ย ย ย ย ย ย color: white;ย
ย ย ย ย ย ย padding: 10px 20px;ย
ย ย ย ย ย ย border: none;ย
ย ย ย ย ย ย border-radius: 5px;ย
ย ย ย ย ย ย cursor: pointer;
ย ย ย ย ย ย margin: 5px;
ย ย ย ย ย ย font-size: 16px;
ย ย ย ย ">๐ Stop Recording</button>
ย ย ย ยย
ย ย ย ย <div id="status" style="margin: 10px; font-weight: bold; color: #666;"></div>
ย ย ย ย <div id="transcript" style="
ย ย ย ย ย ย margin: 10px;ย
ย ย ย ย ย ย padding: 10px;ย
ย ย ย ย ย ย background-color: #f0f0f0;ย
ย ย ย ย ย ย border-radius: 5px;ย
ย ย ย ย ย ย min-height: 40px;
ย ย ย ย ย ย font-style: italic;
ย ย ย ย ">Your transcribed text will appear here...</div>
ย ย ย ยย
ย ย ย ย <button id="sendBtn" onclick="sendToChat()" disabled style="
ย ย ย ย ย ย background-color: #2196F3;ย
ย ย ย ย ย ย color: white;ย
ย ย ย ย ย ย padding: 10px 20px;ย
ย ย ย ย ย ย border: none;ย
ย ย ย ย ย ย border-radius: 5px;ย
ย ย ย ย ย ย cursor: pointer;
ย ย ย ย ย ย margin: 5px;
ย ย ย ย ย ย font-size: 16px;
ย ย ย ย ">๐ค Send to Chat</button>
ย ย ย ยย
ย ย ย ย <button id="clearBtn" onclick="clearTranscript()" style="
ย ย ย ย ย ย background-color: #ff9800;ย
ย ย ย ย ย ย color: white;ย
ย ย ย ย ย ย padding: 10px 20px;ย
ย ย ย ย ย ย border: none;ย
ย ย ย ย ย ย border-radius: 5px;ย
ย ย ย ย ย ย cursor: pointer;
ย ย ย ย ย ย margin: 5px;
ย ย ย ย ย ย font-size: 16px;
ย ย ย ย ">๐ Clear</button>
ย ย </div>

ย ย <script>
ย ย let recognition = null;
ย ย let isRecording = false;
ย ย let finalTranscript = '';

ย ย // Check if browser supports speech recognition
ย ย if ('webkitSpeechRecognition' in window) {
ย ย ย ย recognition = new webkitSpeechRecognition();
ย ย } else if ('SpeechRecognition' in window) {
ย ย ย ย recognition = new SpeechRecognition();
ย ย }

ย ย if (recognition) {
ย ย ย ย recognition.continuous = true;
ย ย ย ย recognition.interimResults = true;
ย ย ย ย recognition.lang = 'en-US';

ย ย ย ย recognition.onstart = function() {
ย ย ย ย ย ย isRecording = true;
ย ย ย ย ย ย document.getElementById('startBtn').disabled = true;
ย ย ย ย ย ย document.getElementById('stopBtn').disabled = false;
ย ย ย ย ย ย document.getElementById('status').innerHTML = '๐ด Recording... Speak now!';
ย ย ย ย ย ย document.getElementById('status').style.color = '#f44336';
ย ย ย ย };

ย ย ย ย recognition.onresult = function(event) {
ย ย ย ย ย ย let interimTranscript = '';
ย ย ย ย ย ยย
ย ย ย ย ย ย for (let i = event.resultIndex; i < event.results.length; i++) {
ย ย ย ย ย ย ย ย const transcript = event.results[i][0].transcript;
ย ย ย ย ย ย ย ย if (event.results[i].isFinal) {
ย ย ย ย ย ย ย ย ย ย finalTranscript += transcript + ' ';
ย ย ย ย ย ย ย ย } else {
ย ย ย ย ย ย ย ย ย ย interimTranscript += transcript;
ย ย ย ย ย ย ย ย }
ย ย ย ย ย ย }
ย ย ย ย ย ยย
ย ย ย ย ย ย document.getElementById('transcript').innerHTML =ย
ย ย ย ย ย ย ย ย finalTranscript + '<span style="color: #999;">' + interimTranscript + '</span>';
ย ย ย ย };

ย ย ย ย recognition.onerror = function(event) {
ย ย ย ย ย ย document.getElementById('status').innerHTML = 'โ Error: ' + event.error;
ย ย ย ย ย ย document.getElementById('status').style.color = '#f44336';
ย ย ย ย ย ย resetButtons();
ย ย ย ย };

ย ย ย ย recognition.onend = function() {
ย ย ย ย ย ย isRecording = false;
ย ย ย ย ย ย resetButtons();
ย ย ย ย ย ย if (finalTranscript.trim() !== '') {
ย ย ย ย ย ย ย ย document.getElementById('sendBtn').disabled = false;
ย ย ย ย ย ย ย ย document.getElementById('status').innerHTML = 'โ Recording completed!';
ย ย ย ย ย ย ย ย document.getElementById('status').style.color = '#4CAF50';
ย ย ย ย ย ย } else {
ย ย ย ย ย ย ย ย document.getElementById('status').innerHTML = 'โ No speech detected';
ย ย ย ย ย ย ย ย document.getElementById('status').style.color = '#ff9800';
ย ย ย ย ย ย }
ย ย ย ย };
ย ย } else {
ย ย ย ย document.getElementById('status').innerHTML = 'โ Speech recognition not supported in this browser';
ย ย ย ย document.getElementById('startBtn').disabled = true;
ย ย }

ย ย function startRecording() {
ย ย ย ย if (recognition && !isRecording) {
ย ย ย ย ย ย finalTranscript = '';
ย ย ย ย ย ย document.getElementById('transcript').innerHTML = 'Listening...';
ย ย ย ย ย ย document.getElementById('sendBtn').disabled = true;
ย ย ย ย ย ย recognition.start();
ย ย ย ย }
ย ย }

ย ย function stopRecording() {
ย ย ย ย if (recognition && isRecording) {
ย ย ย ย ย ย recognition.stop();
ย ย ย ย }
ย ย }

ย ย function resetButtons() {
ย ย ย ย document.getElementById('startBtn').disabled = false;
ย ย ย ย document.getElementById('stopBtn').disabled = true;
ย ย }

ย ย function sendToChat() {
ย ย ย ย if (finalTranscript.trim() !== '') {
ย ย ย ย ย ย // Store in session storage for Streamlit to pick up
ย ย ย ย ย ย parent.sessionStorage.setItem('voice_input', finalTranscript.trim());
ย ย ย ย ย ยย
ย ย ย ย ย ย document.getElementById('status').innerHTML = '๐ค Sent to chat!';
ย ย ย ย ย ย document.getElementById('status').style.color = '#4CAF50';
ย ย ย ย }
ย ย }

ย ย function clearTranscript() {
ย ย ย ย finalTranscript = '';
ย ย ย ย document.getElementById('transcript').innerHTML = 'Your transcribed text will appear here...';
ย ย ย ย document.getElementById('sendBtn').disabled = true;
ย ย ย ย document.getElementById('status').innerHTML = '';
ย ย ย ย parent.sessionStorage.removeItem('voice_input');
ย ย }
ย ย </script>
ย ย """
ย ย return voice_html

# Utility Functions
@st.cache_data
def encode_image(image_path):
ย ย """Encode image to base64."""
ย ย with open(image_path, "rb") as image_file:
ย ย ย ย return base64.b64encode(image_file.read()).decode('utf-8')

def analyze_image_with_query(query, model, encoded_image, api_key):
ย ย """Analyze image using GROQ API."""
ย ย client = Groq(api_key=api_key)
ย ย messages = [
ย ย ย ย {
ย ย ย ย ย ย "role": "user",
ย ย ย ย ย ย "content": [
ย ย ย ย ย ย ย ย {
ย ย ย ย ย ย ย ย ย ย "type": "text",ย
ย ย ย ย ย ย ย ย ย ย "text": query
ย ย ย ย ย ย ย ย },
ย ย ย ย ย ย ย ย {
ย ย ย ย ย ย ย ย ย ย "type": "image_url",
ย ย ย ย ย ย ย ย ย ย "image_url": {
ย ย ย ย ย ย ย ย ย ย ย ย "url": f"data:image/jpeg;base64,{encoded_image}",
ย ย ย ย ย ย ย ย ย ย },
ย ย ย ย ย ย ย ย },
ย ย ย ย ย ย ],
ย ย ย ย }
ย ย ]
ย ยย
ย ย chat_completion = client.chat.completions.create(
ย ย ย ย messages=messages,
ย ย ย ย model=model
ย ย )
ย ยย
ย ย return chat_completion.choices[0].message.content

def transcribe_with_groq(stt_model, audio_filepath, api_key):
ย ย """Transcribe audio using GROQ API."""
ย ย client = Groq(api_key=api_key)
ย ยย
ย ย with open(audio_filepath, "rb") as audio_file:
ย ย ย ย transcription = client.audio.transcriptions.create(
ย ย ย ย ย ย model=stt_model,
ย ย ย ย ย ย file=audio_file,
ย ย ย ย ย ย language="en"
ย ย ย ย )
ย ยย
ย ย return transcription.text

def text_to_speech_with_gtts(input_text, output_filepath):
ย ย """Convert text to speech using gTTS."""
ย ย language = "en"
ย ยย
ย ย audioobj = gTTS(
ย ย ย ย text=input_text,
ย ย ย ย lang=language,
ย ย ย ย slow=False
ย ย )
ย ย audioobj.save(output_filepath)
ย ย return output_filepath




def text_to_speech_with_elevenlabs(input_text, output_filepath, api_key):
ย ย try:
ย ย ย ย from elevenlabs.client import ElevenLabs
ย ย ย ย client = ElevenLabs(api_key=api_key)

ย ย ย ย response = client.text_to_speech.convert(
ย ย ย ย ย ย voice_id="Aria",ย # change to valid voice
ย ย ย ย ย ย model_id="eleven_turbo_v2",
ย ย ย ย ย ย text=input_text
ย ย ย ย )

ย ย ย ย with open(output_filepath, "wb") as f:
ย ย ย ย ย ย for chunk in response:
ย ย ย ย ย ย ย ย f.write(chunk)

ย ย ย ย return output_filepath
ย ย except Exception as e:
ย ย ย ย st.error(f"ElevenLabs TTS failed: {e}")
ย ย ย ย return text_to_speech_with_gtts(input_text, output_filepath)





class DocumentProcessor:
ย ย """Handles PDF loading and processing"""
ย ยย
ย ย @staticmethod
ย ย def load_pdf_files(data_path):
ย ย ย ย """Load PDF files from directory"""
ย ย ย ย if not os.path.exists(data_path):
ย ย ย ย ย ย st.error(f"Data directory '{data_path}' not found!")
ย ย ย ย ย ย return []
ย ย ย ย ย ยย
ย ย ย ย loader = DirectoryLoader(
ย ย ย ย ย ย data_path,
ย ย ย ย ย ย glob='*.pdf',
ย ย ย ย ย ย loader_cls=PyPDFLoader
ย ย ย ย )
ย ย ย ย documents = loader.load()
ย ย ย ย return documents
ย ยย
ย ย @staticmethod
ย ย def create_chunks(documents):
ย ย ย ย """Split documents into chunks"""
ย ย ย ย text_splitter = RecursiveCharacterTextSplitter(
ย ย ย ย ย ย chunk_size=500,
ย ย ย ย ย ย chunk_overlap=50
ย ย ย ย )
ย ย ย ย text_chunks = text_splitter.split_documents(documents)
ย ย ย ย return text_chunks
ย ยย
ย ย @staticmethod
ย ย def create_vectorstore(text_chunks):
ย ย ย ย """Create and save FAISS vectorstore"""
ย ย ย ย embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
ย ย ย ยย
ย ย ย ย # Create vectorstore directory if it doesn't exist
ย ย ย ย os.makedirs(os.path.dirname(DB_FAISS_PATH), exist_ok=True)
ย ย ย ยย
ย ย ย ย db = FAISS.from_documents(text_chunks, embedding_model)
ย ย ย ย db.save_local(DB_FAISS_PATH)
ย ย ย ยย
ย ย ย ย st.success(f"Vectorstore created successfully with {len(text_chunks)} chunks!")
ย ย ย ย return db



class RAGChatbot:
ย ย """Main RAG chatbot class"""
ย ยย
ย ย def __init__(self):
ย ย ย ย self.vectorstore = None
ย ย ย ย self.qa_chain = None
ย ย ย ย self.setup_chain()

ย ยย
ย ย @st.cache_resource
ย ย def get_vectorstore(_self):
ย ย ย ย """Load vectorstore with caching"""
ย ย ย ย if not os.path.exists(DB_FAISS_PATH):
ย ย ย ย ย ย return None
ย ย ย ย ย ยย
ย ย ย ย embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
ย ย ย ย try:
ย ย ย ย ย ย db = FAISS.load_local(
ย ย ย ย ย ย ย ย DB_FAISS_PATH,ย
ย ย ย ย ย ย ย ย embedding_model,ย
ย ย ย ย ย ย ย ย allow_dangerous_deserialization=True
ย ย ย ย ย ย )
ย ย ย ย ย ย return db
ย ย ย ย except Exception as e:
ย ย ย ย ย ย st.error(f"Error loading vectorstore: {str(e)}")
ย ย ย ย ย ย return None
ย ยย
ย ย def setup_chain(self):
ย ย ย ย """Setup the QA chain"""
ย ย ย ย self.vectorstore = self.get_vectorstore()
ย ย ย ยย
ย ย ย ย if self.vectorstore is None:
ย ย ย ย ย ย return
ย ย ย ยย
ย ย ย ย prompt = PromptTemplate(
ย ย ย ย ย ย template=RAG_SYSTEM_PROMPT,ย
ย ย ย ย ย ย input_variables=["context", "question"]
ย ย ย ย )
ย ย ย ยย
ย ย ย ย # Setup Gemini LLM
ย ย ย ย try:
ย ย ย ย ย ย # Get model from session state or default
ย ย ย ย ย ย model_name = getattr(st.session_state, 'selected_model', 'gemini-2.0-flash')
ย ย ย ย ย ย # Use st.secrets to retrieve the API key securely
ย ย ย ย ย ย google_api_key = st.secrets.get("GOOGLE_API_KEY")

ย ย ย ย ย ย if not google_api_key:
ย ย ย ย ย ย ย ย st.error("Google API Key not found in Streamlit Secrets.")
ย ย ย ย ย ย ย ย return

ย ย ย ย ย ย llm = ChatGoogleGenerativeAI(
ย ย ย ย ย ย ย ย model=model_name,
ย ย ย ย ย ย ย ย temperature=0.0,
ย ย ย ย ย ย ย ย google_api_key=google_api_key
ย ย ย ย ย ย )
ย ย ย ย ย ยย
ย ย ย ย ย ย self.qa_chain = RetrievalQA.from_chain_type(
ย ย ย ย ย ย ย ย llm=llm,
ย ย ย ย ย ย ย ย chain_type="stuff",
ย ย ย ย ย ย ย ย retriever=self.vectorstore.as_retriever(search_kwargs={'k': 3}),
ย ย ย ย ย ย ย ย return_source_documents=True,
ย ย ย ย ย ย ย ย chain_type_kwargs={'prompt': prompt}
ย ย ย ย ย ย )
ย ย ย ย ย ยย
ย ย ย ย except Exception as e:
ย ย ย ย ย ย st.error(f"Error setting up Gemini API: {str(e)}")
ย ยย
ย ย def get_response(self, query):
ย ย ย ย """Get response from QA chain"""
ย ย ย ย if self.qa_chain is None:
ย ย ย ย ย ย return "Sorry, the chatbot is not properly initialized. Please check your setup.", []
ย ย ย ยย
ย ย ย ย try:
ย ย ย ย ย ย response = self.qa_chain.invoke({'query': query})
ย ย ย ย ย ย return response["result"], response["source_documents"]
ย ย ย ย except Exception as e:
ย ย ย ย ย ย return f"Error generating response: {str(e)}", []


class VisionProcessor:
ย ย """Handles image analysis and vision processing"""

ย ย def __init__(self, groq_api_key, elevenlabs_api_key=None):
ย ย ย ย # Pass keys from the main function
ย ย ย ย self.groq_api_key = groq_api_key
ย ย ย ย self.elevenlabs_api_key = elevenlabs_api_key
ย ยย
ย ย def analyze_image_with_text(self, image_path, user_query=""):
ย ย ย ย """Analyze image with optional user query"""
ย ย ย ย try:
ย ย ย ย ย ย encoded_image = encode_image(image_path)
ย ย ย ย ย ย full_query = VISION_SYSTEM_PROMPT
ย ย ย ย ย ย if user_query:
ย ย ย ย ย ย ย ย full_query += f"\n\nUser's specific question: {user_query}"
ย ย ย ย ย ยย
ย ย ย ย ย ย response = analyze_image_with_query(
ย ย ย ย ย ย ย ย query=full_query,
ย ย ย ย ย ย ย ย encoded_image=encoded_image,
ย ย ย ย ย ย ย ย model="meta-llama/llama-4-scout-17b-16e-instruct",
ย ย ย ย ย ย ย ย api_key=self.groq_api_key
ย ย ย ย ย ย )
ย ย ย ย ย ย return response
ย ย ย ย except Exception as e:
ย ย ย ย ย ย return f"Error analyzing image: {str(e)}"
ย ยย
ย ย def generate_audio_response(self, text, use_elevenlabs=False):
ย ย ย ย """Generate audio response from text"""
ย ย ย ย try:
ย ย ย ย ย ย with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_audio:
ย ย ย ย ย ย ย ย output_path = temp_audio.name
ย ย ย ย ย ยย
ย ย ย ย ย ย if use_elevenlabs and self.elevenlabs_api_key:
ย ย ย ย ย ย ย ย text_to_speech_with_elevenlabs(text, output_path, self.elevenlabs_api_key)
ย ย ย ย ย ย else:
ย ย ย ย ย ย ย ย text_to_speech_with_gtts(text, output_path)
ย ย ย ย ย ยย
ย ย ย ย ย ย return output_path
ย ย ย ย except Exception as e:
ย ย ย ย ย ย st.error(f"Error generating audio: {e}")
ย ย ย ย ย ย return None

def check_voice_input():
ย ย """Check for voice input from session storage"""
ย ย voice_input_js = """
ย ย <script>
ย ย const voiceInput = sessionStorage.getItem('voice_input');
ย ย if (voiceInput) {
ย ย ย ย sessionStorage.removeItem('voice_input');
ย ย ย ย return voiceInput;
ย ย }
ย ย return null;
ย ย </script>
ย ย """
ย ย return components.html(voice_input_js, height=0)

def main():
ย ย st.set_page_config(
ย ย ย ย page_title="Unified Medical AI Assistant",
ย ย ย ย page_icon="๐ฉบ",
ย ย ย ย layout="wide"
ย ย )
ย ยย
ย ย st.title("๐ฉบ Unified Medical AI Assistant - RAG + Vision + Voice")
ย ย st.markdown("### ๐ค Smart routing: Text/Voice โ RAG | Images โ Vision Analysis")
ย ย st.markdown("---")
ย ยย
ย ย # Sidebar for configuration
ย ย with st.sidebar:
ย ย ย ย st.header("โ Configuration")
ย ย ย ยย
ย ย ย ย # API Keys
ย ย ย ย st.subheader("๐ API Keys")
ย ย ย ยย
ย ย ย ย # Retrieve keys from Streamlit's secrets for display and use
ย ย ย ย google_api_key = st.secrets.get("GOOGLE_API_KEY")
ย ย ย ย groq_api_key = st.secrets.get("GROQ_API_KEY")
ย ย ย ย elevenlabs_api_key = st.secrets.get("ELEVENLABS_API_KEY")
ย ย ย ย 
ย ย ย ย # These text inputs are for display only, to show users if the keys are set
ย ย ย ย st.text_input(
ย ย ย ย ย ย "Google API Key (Gemini):",
ย ย ย ย ย ย value=google_api_key,
ย ย ย ย ย ย type="password",
ย ย ย ย ย ย help="Add this key to Streamlit Cloud Secrets"
ย ย ย ย )
ย ย ย ยย
ย ย ย ย st.text_input(
ย ย ย ย ย ย "GROQ API Key:",
ย ย ย ย ย ย value=groq_api_key,
ย ย ย ย ย ย type="password",
ย ย ย ย ย ย help="Add this key to Streamlit Cloud Secrets"
ย ย ย ย )
ย ย ย ยย
ย ย ย ย st.text_input(
ย ย ย ย ย ย "ElevenLabs API Key (Optional):",
ย ย ย ย ย ย value=elevenlabs_api_key,
ย ย ย ย ย ย type="password",
ย ย ย ย ย ย help="Add this key to Streamlit Cloud Secrets"
ย ย ย ย )
ย ย ย ยย
ย ย ย ย use_elevenlabs = st.checkbox(
ย ย ย ย ย ย "Use ElevenLabs TTS",ย
ย ย ย ย ย ย value=bool(elevenlabs_api_key),
ย ย ย ย ย ย help="Uncheck to use free gTTS instead"
ย ย ย ย )
ย ย ย ยย
ย ย ย ย st.markdown("---")
ย ย ย ยย
ย ย ย ย # RAG Configuration
ย ย ย ย st.subheader("๐ RAG Configuration")
ย ย ย ย vectorstore_exists = os.path.exists(DB_FAISS_PATH)
ย ย ย ยย
ย ย ย ย if vectorstore_exists:
ย ย ย ย ย ย st.success("โ Vectorstore loaded successfully!")
ย ย ย ย else:
ย ย ย ย ย ย st.warning("โ No vectorstore found. Please process documents first.")
ย ย ย ยย
ย ย ย ย if st.button("๐ Process PDF Documents"):
ย ย ย ย ย ย with st.spinner("Processing documents..."):
ย ย ย ย ย ย ย ย documents = DocumentProcessor.load_pdf_files(DATA_PATH)
ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย if not documents:
ย ย ย ย ย ย ย ย ย ย st.error("No PDF files found in the data directory!")
ย ย ย ย ย ย ย ย else:
ย ย ย ย ย ย ย ย ย ย text_chunks = DocumentProcessor.create_chunks(documents)
ย ย ย ย ย ย ย ย ย ย DocumentProcessor.create_vectorstore(text_chunks)
ย ย ย ย ย ย ย ย ย ย st.rerun()
ย ย ย ยย
ย ย ย ย # Model selection
ย ย ย ย model_options = {
ย ย ย ย ย ย "Gemini 2.0 Flash (Recommended)": "gemini-2.0-flash",ย
ย ย ย ย ย ย "Gemini 2.5 Flash": "gemini-2.5-flash",
ย ย ย ย ย ย "Gemini 2.5 Pro": "gemini-2.5-pro"
ย ย ย ย }
ย ย ย ยย
ย ย ย ย selected_model = st.selectbox(
ย ย ย ย ย ย "Choose Gemini Model:",
ย ย ย ย ย ย options=list(model_options.keys())
ย ย ย ย )
ย ย ย ยย
ย ย ย ย # Store selected model in session state
ย ย ย ย if 'selected_model' not in st.session_state:
ย ย ย ย ย ย st.session_state.selected_model = model_options[selected_model]
ย ย ย ยย
ย ย ย ย if st.session_state.selected_model != model_options[selected_model]:
ย ย ย ย ย ย st.session_state.selected_model = model_options[selected_model]
ย ย ย ย ย ย if 'rag_chatbot' in st.session_state:
ย ย ย ย ย ย ย ย del st.session_state.rag_chatbot
ย ย ย ยย
ย ย ย ย st.markdown("---")
ย ย ย ย st.subheader("โน How It Works")
ย ย ย ย st.markdown("""
ย ย ย ย *๐ฏ Smart Routing:*
ย ย ย ย - *Upload Image* โ Vision Analysis (GROQ)
ย ย ย ย - *Text/Voice Input* โ RAG Chatbot (Gemini)
ย ย ย ยย
ย ย ย ย *๐ Setup:*
ย ย ย ย 1. Add API keys above
ย ย ย ย 2. Process PDF documents for RAG
ย ย ย ย 3. Use voice, text, or images to interact
ย ย ย ย """)
ย ยย
ย ย # Check API keys
ย ย if not google_api_key:
ย ย ย ย st.error("Please provide your Google API key in the Streamlit Cloud secrets for RAG functionality.")
ย ยย
ย ย if not groq_api_key:
ย ย ย ย st.error("Please provide your GROQ API key in the Streamlit Cloud secrets for vision analysis.")
ย ยย
ย ย # Main interface
ย ย col1, col2 = st.columns([1, 1])
ย ยย
ย ย with col1:
ย ย ย ย st.header("๐ค Voice Input")
ย ย ย ย components.html(create_voice_input_component(), height=280)
ย ยย
ย ย with col2:
ย ย ย ย st.header("๐ธ Image Upload")
ย ย ย ย uploaded_image = st.file_uploader(
ย ย ย ย ย ย "Upload medical image for analysis",ย
ย ย ย ย ย ย type=['png', 'jpg', 'jpeg'],
ย ย ย ย ย ย help="Upload triggers Vision Analysis (bypasses RAG)"
ย ย ย ย )
ย ย ย ยย
ย ย ย ย if uploaded_image:
ย ย ย ย ย ย st.image(uploaded_image, caption="Uploaded Image", use_container_width=True)
ย ยย
ย ย # Initialize components
ย ย if google_api_key and 'rag_chatbot' not in st.session_state:
ย ย ย ย vectorstore_exists = os.path.exists(DB_FAISS_PATH)
ย ย ย ย if vectorstore_exists:
ย ย ย ย ย ย st.session_state.rag_chatbot = RAGChatbot()
ย ยย
ย ย if groq_api_key and 'vision_processor' not in st.session_state:
ย ย ย ย st.session_state.vision_processor = VisionProcessor(groq_api_key, elevenlabs_api_key)
ย ยย
ย ย # Initialize chat messages
ย ย if 'messages' not in st.session_state:
ย ย ย ย st.session_state.messages = []
ย ยย
ย ย # Check for voice input
ย ย voice_input = None
ย ย try:
ย ย ย ย if st.button("๐ Check Voice Input", help="Click to check if voice input is available"):
ย ย ย ย ย ย pass
ย ย except:
ย ย ย ย pass
ย ยย
ย ย st.markdown("---")
ย ย st.header("๐ฌ Chat Interface")
ย ยย
ย ย # Display chat messages
ย ย for message in st.session_state.messages:
ย ย ย ย with st.chat_message(message['role']):
ย ย ย ย ย ย if message.get('type') == 'image_analysis':
ย ย ย ย ย ย ย ย st.markdown("๐ผ *Image Analysis Result:*")
ย ย ย ย ย ย st.markdown(message['content'])
ย ย ย ย ย ยย
ย ย ย ย ย ย # Display audio if available
ย ย ย ย ย ย if message.get('audio_path') and os.path.exists(message['audio_path']):
ย ย ย ย ย ย ย ย st.audio(message['audio_path'], format="audio/mp3")
ย ยย
ย ย # Process uploaded image immediately
ย ย if uploaded_image and groq_api_key:
ย ย ย ย st.markdown("### ๐ Processing Image...")
ย ย ย ยย
ย ย ย ย with st.spinner("Analyzing image..."):
ย ย ย ย ย ย try:
ย ย ย ย ย ย ย ย # Save uploaded image to temporary file
ย ย ย ย ย ย ย ย with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as temp_img:
ย ย ย ย ย ย ย ย ย ย temp_img.write(uploaded_image.read())
ย ย ย ย ย ย ย ย ย ย temp_img_path = temp_img.name
ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย # Analyze image (Vision path - bypasses RAG)
ย ย ย ย ย ย ย ย vision_response = st.session_state.vision_processor.analyze_image_with_text(temp_img_path)
ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย # Generate audio response
ย ย ย ย ย ย ย ย audio_path = None
ย ย ย ย ย ย ย ย if use_elevenlabs:
ย ย ย ย ย ย ย ย ย ย audio_path = st.session_state.vision_processor.generate_audio_response(
ย ย ย ย ย ย ย ย ย ย ย ย vision_response, use_elevenlabs=True
ย ย ย ย ย ย ย ย ย ย )
ย ย ย ย ย ย ย ย else:
ย ย ย ย ย ย ย ย ย ย audio_path = st.session_state.vision_processor.generate_audio_response(
ย ย ย ย ย ย ย ย ย ย ย ย vision_response, use_elevenlabs=False
ย ย ย ย ย ย ย ย ย ย )
ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย # Add to chat
ย ย ย ย ย ย ย ย st.session_state.messages.append({
ย ย ย ย ย ย ย ย ย ย 'role': 'user',
ย ย ย ย ย ย ย ย ย ย 'content': f"๐ธ Uploaded image: {uploaded_image.name}",
ย ย ย ย ย ย ย ย ย ย 'type': 'image_upload'
ย ย ย ย ย ย ย ย })
ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย st.session_state.messages.append({
ย ย ย ย ย ย ย ย ย ย 'role': 'assistant',
ย ย ย ย ย ย ย ย ย ย 'content': vision_response,
ย ย ย ย ย ย ย ย ย ย 'type': 'image_analysis',
ย ย ย ย ย ย ย ย ย ย 'audio_path': audio_path
ย ย ย ย ย ย ย ย })
ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย # Clean up temp file
ย ย ย ย ย ย ย ย os.unlink(temp_img_path)
ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย # Reset uploaded image to prevent reprocessing
ย ย ย ย ย ย ย ย st.session_state.uploaded_image_processed = True
ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย except Exception as e:
ย ย ย ย ย ย ย ย st.error(f"Error processing image: {str(e)}")
ย ยย
ย ย # Chat input for text/voice
ย ย if prompt := st.chat_input("Ask medical questions (text input) or use voice input above..."):
ย ย ย ย # Add user message
ย ย ย ย st.session_state.messages.append({'role': 'user', 'content': prompt})
ย ย ย ยย
ย ย ย ย with st.chat_message('user'):
ย ย ย ย ย ย st.markdown(prompt)
ย ย ย ยย
ย ย ย ย # Process with RAG (text input path)
ย ย ย ย if 'rag_chatbot' in st.session_state and vectorstore_exists:
ย ย ย ย ย ย with st.chat_message('assistant'):
ย ย ย ย ย ย ย ย with st.spinner("Analyzing medical information..."):
ย ย ย ย ย ย ย ย ย ย result, source_docs = st.session_state.rag_chatbot.get_response(prompt)
ย ย ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย ย ย # Display result
ย ย ย ย ย ย ย ย ย ย st.markdown(result)
ย ย ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย ย ย # Display source documents if available
ย ย ย ย ย ย ย ย ย ย if source_docs:
ย ย ย ย ย ย ย ย ย ย ย ย with st.expander("๐ Source Documents"):
ย ย ย ย ย ย ย ย ย ย ย ย ย ย for i, doc in enumerate(source_docs, 1):
ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย st.markdown(f"*Source {i}:*")
ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย st.text(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย if hasattr(doc, 'metadata') and doc.metadata:
ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย st.json(doc.metadata)
ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย st.markdown("---")
ย ย ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย ย ย # Prepare content for session state
ย ย ย ย ย ย ย ย ย ย content_with_sources = result
ย ย ย ย ย ย ย ย ย ย if source_docs:
ย ย ย ย ย ย ย ย ย ย ย ย content_with_sources += f"\n\n*Sources:* {len(source_docs)} document(s) referenced"
ย ย ย ย ย ย ย ย ย ยย
ย ย ย ย ย ย ย ย ย ย st.session_state.messages.append({
ย ย ย ย ย ย ย ย ย ย ย ย 'role': 'assistant',ย
ย ย ย ย ย ย ย ย ย ย ย ย 'content': content_with_sources,
ย ย ย ย ย ย ย ย ย ย ย ย 'type': 'rag_response'
ย ย ย ย ย ย ย ย ย ย })
ย ย ย ย else:
ย ย ย ย ย ย st.error("RAG chatbot not available. Please check your configuration and ensure documents are processed.")
ย ยย
ย ย # Instructions
ย ย st.markdown("---")
ย ย st.info("""
ย ย *๐ฏ Smart Usage Guide:*
ย ยย
ย ย *For Vision Analysis (Image + AI Doctor):*
ย ย - Upload any medical image above
ย ย - System automatically uses GROQ vision model
ย ย - Get instant AI doctor analysis with voice response
ย ยย
ย ย *For RAG Chatbot (Text + Documents):*
ย ย - Type questions in the chat or use voice input
ย ย - System searches your uploaded PDF documents
ย ย - Get comprehensive answers from your medical database
ย ยย
ย ย *Voice Input:*
ย ย - Click ๐ค "Start Recording" โ Speak โ "Stop Recording" โ "Send to Chat"
ย ย - Works with both RAG and Vision modes
ย ย """)
ย ยย
ย ย # Footer
ย ย st.markdown("---")
ย ย st.markdown(
ย ย ย ย "<div style='text-align: center; color: gray;'>"
ย ย ย ย "โ Disclaimer: This is for educational purposes only. Always consult a real healthcare professional for medical advice."
ย ย ย ย "</div>",ย
ย ย ย ย unsafe_allow_html=True
ย ย )

if __name__ == "__main__":
ย ย main()
